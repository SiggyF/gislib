# -*- coding: utf-8 -*-
# (c) Nelen & Schuurmans.  GPL licensed, see LICENSE.rst.

from __future__ import print_function
from __future__ import unicode_literals
from __future__ import absolute_import
from __future__ import division

import collections
import pickle
import multiprocessing

from gislib.store import datasets


def process(job):
    """
    Use the datasets generated by adapter to update location from store.
    """
    target = job['store'].get_dataset(job['location'])
    for source in job['adapter'].get_datasets():
        datasets.reproject(source, target)
    job['store'].put_dataset(job['location'], target)


class Store(object):
    """
    Raster object with optional arguments including.
    """

    FRAME = 'frame'

    def __init__(self, storage, frame=None):
        """
        Separate schemas in the storage are placed as attributes on
        the store.
        """
        # Init schemas
        for schema in ('databox', 'metabox', 'config', 'metadata'):
            split = False if schema == 'config' else True
            setattr(self, schema, storage.get_schema(schema, split=split))

        # Add a schema for each aggregator when they are added.
        if frame is None:
            self.frame = pickle.loads(self.config[self.FRAME])
            return

        # Write config
        self.verify_not_initialized()
        self.config[self.FRAME] = pickle.dumps(frame)
        self.frame = frame

    def verify_not_initialized(self):
        """ If the store already has a structure, raise an exception. """
        try:
            self.config[self.FRAME]
        except KeyError:
            return  # That's expected.
        raise IOError('Store already has a structure!')


    def get_dataset(self, location):
        """
        Return dataset or location.
        """
        try:
            string = self.databox[location.key]
            return self.frame.get_saved_dataset(string=string)
        except ValueError:
            return self.frame.get_location(string=string)
        except KeyError:
            return self.frame.get_empty_dataset(location=location)

    def put_dataset(self, location, dataset):
        """
        Write dataset to location.
        """
        self.databox[location.key] = dataset.tostring()

    def add_from(self, adapter):
        """
        Put data in the store from an iterable of datasets.

        This must employ multiprocessing. So what do we do?

        First make lists per location of the sourcepaths
        Then, using multiprocessing, for each location put all those datasets in.
        """
        # Sort jobs
        sourcepaths = collections.defaultdict(list)
        for sourcepath, config in adapter.get_configs():
            for location in self.frame.get_locations(config=config):
                print(location)
                sourcepaths[location].append(sourcepath)

        # Add using multiprocessing
        Adapter = adapter.__class__
        jobs = (dict(store=self,
                     location=location,
                     adapter=Adapter(sourcepaths=sourcepaths[location]))
                for location in sourcepaths)
        #pool = multiprocessing.Pool()
        #pool.map(process, jobs)
        #pool.close()
        #for job in jobs:
            #process(job)
            

    def fill_into(self, dataset):
        """
        Fill dataset with data from the store.
        """
        pass
    
